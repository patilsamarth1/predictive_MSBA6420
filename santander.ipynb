{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-04-21T06:03:04.734031Z","iopub.status.busy":"2022-04-21T06:03:04.733523Z","iopub.status.idle":"2022-04-21T06:03:13.017449Z","shell.execute_reply":"2022-04-21T06:03:13.01662Z","shell.execute_reply.started":"2022-04-21T06:03:04.73394Z"},"trusted":true},"outputs":[],"source":["import os\n","import warnings\n","import pandas as pd\n","import numpy as np\n","import lightgbm as lgb\n","import statsmodels.api as sm\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import gc\n","from skopt.space import Integer\n","from skopt.space import Real\n","from skopt.space import Categorical\n","from skopt.utils import use_named_args\n","from skopt import gp_minimize\n","from tensorflow import keras\n","from tqdm import tqdm\n","from bayes_opt import BayesianOptimization\n","from skopt  import BayesSearchCV \n","from sklearn.metrics import roc_auc_score, recall_score\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from scipy.special import logit\n","\n","SEED = 29\n","MANUAL_RUN = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'Localhost') == 'Interactive'\n","\n","warnings.simplefilter(action='ignore', category=FutureWarning)"]},{"cell_type":"markdown","metadata":{},"source":["**Unused**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T16:01:55.895364Z","iopub.status.busy":"2022-04-17T16:01:55.895082Z","iopub.status.idle":"2022-04-17T16:01:55.905036Z","shell.execute_reply":"2022-04-17T16:01:55.90403Z","shell.execute_reply.started":"2022-04-17T16:01:55.895336Z"},"trusted":true},"outputs":[],"source":["if MANUAL_RUN:\n","# TAKEN FROM somang1418'S KERNEL\n","# https://www.kaggle.com/code/somang1418/tuning-hyperparameters-under-10-minutes-lgbm/notebook\n","        \n","    def reduce_mem(df, verbose=True):\n","        numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n","        start_mem = df.memory_usage().sum() / 1024**2\n","        i = 0 \n","        for col in df.columns:\n","            col_type = df[col].dtypes\n","            if col_type in numerics:\n","                i += 1\n","                c_min = df[col].min()\n","                c_max = df[col].max()\n","                if str(col_type)[:3] == 'int':                    \n","                    if c_max<=255:\n","                        df[col] = df[col].astype('uint8')\n","                    else:\n","                        df[col] = df[col].astype('uint16')  \n","        end_mem = df.memory_usage().sum() / 1024**2\n","        if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n","        print('Total colunms changed: ', i)\n","        return df"]},{"cell_type":"markdown","metadata":{},"source":["**Importing the data**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-21T06:03:25.846726Z","iopub.status.busy":"2022-04-21T06:03:25.846197Z","iopub.status.idle":"2022-04-21T06:03:44.37621Z","shell.execute_reply":"2022-04-21T06:03:44.375462Z","shell.execute_reply.started":"2022-04-21T06:03:25.846687Z"},"trusted":true},"outputs":[],"source":["# train_df = reduce_mem(pd.read_csv('../input/santander-customer-transaction-prediction/train.csv'))\n","# test_df = reduce_mem(pd.read_csv('../input/santander-customer-transaction-prediction/test.csv'))\n","\n","train_df = pd.read_csv('../input/santander-customer-transaction-prediction/train.csv')\n","test_df = pd.read_csv('../input/santander-customer-transaction-prediction/test.csv')"]},{"cell_type":"markdown","metadata":{},"source":["Checking the distribution of target variable"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_df['target'].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["Checking for null values in the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T20:22:14.746501Z","iopub.status.busy":"2022-04-17T20:22:14.74577Z","iopub.status.idle":"2022-04-17T20:22:14.949366Z","shell.execute_reply":"2022-04-17T20:22:14.948623Z","shell.execute_reply.started":"2022-04-17T20:22:14.746464Z"},"trusted":true},"outputs":[],"source":["train_df.isnull().sum().sum() + test_df.isnull().sum().sum()"]},{"cell_type":"markdown","metadata":{},"source":["Getting the indexes of the real and synthetic data in the test dataset. It was important to use this to improve the final score. Taking all test data instead of only the real ones would give inaccurate value counts, thereby decreasing the model performance."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-21T06:03:44.378132Z","iopub.status.busy":"2022-04-21T06:03:44.377889Z","iopub.status.idle":"2022-04-21T06:03:50.628011Z","shell.execute_reply":"2022-04-21T06:03:50.627259Z","shell.execute_reply.started":"2022-04-21T06:03:44.378099Z"},"trusted":true},"outputs":[],"source":["# TAKEN FROM YAG320'S KERNEL\n","# https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split\n","\n","df_test = test_df.copy()\n","df_test.drop(['ID_code'], axis=1, inplace=True)\n","df_test = df_test.values\n","\n","unique_samples = []\n","unique_count = np.zeros_like(df_test)\n","for feature in tqdm(range(df_test.shape[1])):\n","    _, index_, count_ = np.unique(df_test[:, feature], return_counts=True, return_index=True)\n","    unique_count[index_[count_ == 1], feature] += 1\n","\n","# Samples which have unique values are real the others are fake\n","real_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\n","synthetic_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n","\n","del df_test, count_, index_, unique_count\n","\n","print(len(real_samples_indexes))\n","print(len(synthetic_samples_indexes))"]},{"cell_type":"markdown","metadata":{},"source":["**Feature Engineering**\n","\n","Multiple combinations of feature engineering were tried. However, the best one was adding 200 new features representing the value counts for each of the original 200 features."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-21T06:03:50.629911Z","iopub.status.busy":"2022-04-21T06:03:50.629347Z","iopub.status.idle":"2022-04-21T06:03:57.870565Z","shell.execute_reply":"2022-04-21T06:03:57.869733Z","shell.execute_reply.started":"2022-04-21T06:03:50.62987Z"},"trusted":true},"outputs":[],"source":["orig_features = [col for col in train_df.columns if col.startswith('var')]\n","df_all = pd.concat([train_df, test_df.iloc[real_samples_indexes]])\n","\n","for i, feature in tqdm(enumerate(orig_features)):\n","        \n","    temp = df_all[feature].value_counts()\n","    \n","    new_feat = feature + 'new'\n","    \n","    train_df[new_feat] = train_df[feature].map(temp)\n","    test_df[new_feat] = test_df[feature].map(temp)\n","    \n","#     if temp.max()<=255:\n","#         train_df[new_feat] = train_df[new_feat].astype('uint8')\n","#         test_df[new_feat] = test_df[new_feat].astype('uint8')\n","#     else:\n","#         train_df[new_feat] = train_df[new_feat].astype('uint16')\n","#         test_df[new_feat] = test_df[new_feat].astype('uint16')    \n","        \n","#     new_feat = feature + 'cat_count'\n","\n","#     train_df[new_feat] = train_df[feature].map(temp).map(lambda x: min(50, x)).astype(np.uint8)\n","#     test_df[new_feat] = test_df[feature].map(temp).map(lambda x: min(50, x)).astype(np.uint8)\n","    \n","#     pick min(10,x) where x is the value count - output [1,10]\n","#     train_df[new_feat] = train_df[feature].map(temp).map(lambda x: min(10, x)).astype(np.uint8)\n","#     test_df[new_feat] = test_df[feature].map(temp).map(lambda x: min(10, x)).astype(np.uint8)\n","\n","# #     0 if unique, x-mean if non-unique      ((x - mean)*(0 if unique, 1 if not-unique))\n","#     train_df[feature + 'sum'] = ((train_df[feature] - df_all[feature].mean()) * train_df[feature + 'vc'].map(lambda x: int(x > 1))).astype(np.float32)\n","#     test_df[feature + 'sum'] = ((test_df[feature] - df_all[feature].mean()) * test_df[feature + 'vc'].map(lambda x: int(x > 1))).astype(np.float32) \n","    \n","# #     0 if value_count < 3, otherwise x\n","#     train_df[feature + 'sum2'] = ((train_df[feature]) * train_df[feature + 'vc'].map(lambda x: int(x > 2))).astype(np.float32)\n","#     test_df[feature + 'sum2'] = ((test_df[feature]) * test_df[feature + 'vc'].map(lambda x: int(x > 2))).astype(np.float32)\n","\n","# #     0 if value_count < 5, otherwise x\n","#     train_df[feature + 'sum3'] = ((train_df[feature]) * train_df[feature + 'vc'].map(lambda x: int(x > 4))).astype(np.float32) \n","#     test_df[feature + 'sum3'] = ((test_df[feature]) * test_df[feature + 'vc'].map(lambda x: int(x > 4))).astype(np.float32)\n","    \n","del df_all, temp\n","print('done')"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-04-19T14:07:05.376344Z","iopub.status.busy":"2022-04-19T14:07:05.376036Z","iopub.status.idle":"2022-04-19T14:07:05.38152Z","shell.execute_reply":"2022-04-19T14:07:05.3806Z","shell.execute_reply.started":"2022-04-19T14:07:05.376311Z"}},"source":["**Hyperparameter Tuning**\n","\n","For custom tuning skopt package was used. Since the tuning time was large, the hyperparameter space was manually split into multiple iterations - with each iteration searching a narrower space than before."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-21T06:09:08.544233Z","iopub.status.busy":"2022-04-21T06:09:08.54396Z","iopub.status.idle":"2022-04-21T06:09:08.568277Z","shell.execute_reply":"2022-04-21T06:09:08.567506Z","shell.execute_reply.started":"2022-04-21T06:09:08.544203Z"},"trusted":true},"outputs":[],"source":["num_vars = len(orig_features)\n","n_splits = 4\n","space  = [Categorical([0.1, 0.25, 0.5], name='learning_rate'),\n","          Categorical([1.9, 2.0, 2.1, 2.2], name='reg_alpha'),\n","          Categorical([0, 0.1, 0.2, 0.3], name='reg_lambda'),\n","          Integer(3, 8, name='num_leaves')]\n","    \n","@use_named_args(space)\n","def evaluate_model(**param):\n","    \n","    param['objective'] = 'binary'\n","    param['feature_fraction'] = 1.0\n","    param['metric'] = 'auc'\n","    param['boost_from_average'] = False\n","    param['boosting_type'] = 'gbdt'\n","    param['max_depth'] = -1\n","    param['early_stopping_round'] = 50\n","    param['seed'] = SEED\n","    param['is_unbalance'] = True\n","    param['num_threads'] = 8\n","    param['verbosity'] = -1 \n","\n","    oof = pd.DataFrame(np.zeros((test_df.shape[0], len(orig_features))), columns = orig_features)\n","    \n","    skf = StratifiedKFold(n_splits, shuffle=True, random_state=SEED)\n","\n","    for fold, (train_ind, val_ind) in enumerate(skf.split(train_df, train_df.target.values)):\n","\n","        train = train_df.iloc[train_ind]\n","        valid = train_df.iloc[val_ind]\n","\n","        for i in range(num_vars):\n","            feature = 'var_' + str(i)\n","            feat_choices = [feature, feature + 'new']\n","\n","            train_data  = lgb.Dataset(train[feat_choices], label=train['target'])\n","            val_data = lgb.Dataset(valid[feat_choices], label=valid['target'])     \n","\n","            model = lgb.train(param, train_data, valid_sets = [train_data, val_data], \n","                    verbose_eval=False)\n","\n","            best_iter = model.best_iteration\n","\n","            oof[feature].iloc[val_ind] = model.predict(valid[feat_choices], num_iteration=best_iter)\n","\n","    preds = logit(oof)\n","    preds = np.sum(oof[[col for col in oof.columns if col.startswith('var')]].values, axis=1)\n","    \n","    return -roc_auc_score(train_df[['target']],preds)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-21T06:09:10.003228Z","iopub.status.busy":"2022-04-21T06:09:10.002976Z"},"trusted":true},"outputs":[],"source":["if MANUAL_RUN:\n","    result = gp_minimize(evaluate_model, space, random_state = SEED, verbose = True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if MANUAL_RUN:\n","    print(result.x)\n","    print('min auc: ', result.fun)"]},{"cell_type":"markdown","metadata":{},"source":["Using the tuned hyperparamters to train the 200 lgb models"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T21:18:32.062231Z","iopub.status.busy":"2022-04-18T21:18:32.061441Z","iopub.status.idle":"2022-04-18T21:18:32.070836Z","shell.execute_reply":"2022-04-18T21:18:32.06999Z","shell.execute_reply.started":"2022-04-18T21:18:32.062183Z"},"trusted":true},"outputs":[],"source":["param = {\n","    'boost_from_average':'false',\n","    'boost': 'gbdt',\n","    'feature_fraction': 1.0,\n","    'learning_rate': 0.1,\n","    'max_depth': -1,\n","    'metric': 'auc',\n","    'num_leaves': 3,\n","    'num_threads': 8,\n","    'objective': 'binary',\n","    'reg_alpha': 2,\n","    'reg_lambda': 0.1,\n","    'verbosity': -1,\n","    'is_unbalance' : True,\n","    'seed': SEED,\n","    'early_stopping_round': 50\n","}\n","    "]},{"cell_type":"markdown","metadata":{},"source":["Running k-fold cross-validation for training 200 models for the 200 original features"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T21:20:44.6087Z","iopub.status.busy":"2022-04-18T21:20:44.608393Z","iopub.status.idle":"2022-04-18T21:20:45.021614Z","shell.execute_reply":"2022-04-18T21:20:45.020823Z","shell.execute_reply.started":"2022-04-18T21:20:44.608666Z"},"trusted":true},"outputs":[],"source":["oof = pd.DataFrame(np.zeros((test_df.shape[0], len(orig_features))), columns = orig_features)\n","# oof['var_ones'] = 1\n","oof['ID_code'] = train_df[['ID_code']]\n","oof['target'] = train_df[['target']]\n","\n","predictions = pd.DataFrame(np.zeros((test_df.shape[0], len(orig_features))), columns = orig_features)\n","predictions['ID_code'] = test_df[['ID_code']]\n","# predictions['var_ones'] = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T21:20:46.848074Z","iopub.status.busy":"2022-04-18T21:20:46.847317Z","iopub.status.idle":"2022-04-18T21:39:55.359974Z","shell.execute_reply":"2022-04-18T21:39:55.359296Z","shell.execute_reply.started":"2022-04-18T21:20:46.848028Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["num_vars = len(orig_features)\n","num_iter = 2000\n","\n","n_splits = 5\n","skf = StratifiedKFold(n_splits, shuffle=True, random_state=SEED)\n","\n","for fold, (train_ind, val_ind) in enumerate(skf.split(train_df, train_df.target.values)):\n","    \n","    train = train_df.iloc[train_ind]\n","    valid = train_df.iloc[val_ind]\n","    \n","    for i in range(num_vars):\n","        feature = 'var_' + str(i)\n","#         feat_choices = [feature,  feature + 'new',  feature + 'vc', feature + 'sum', feature + 'sum2', feature + 'sum3']\n","        feat_choices = [feature, feature + 'new']\n","        \n","        train_data  = lgb.Dataset(train[feat_choices], label=train['target'])\n","        val_data = lgb.Dataset(valid[feat_choices], label=valid['target'])     \n","\n","        model = lgb.train(param, train_data, num_iter, valid_sets = [train_data, val_data], verbose_eval=False)\n","        \n","        best_iter = model.best_iteration\n","        \n","#         oof.iloc[val_ind][feature] assignment does not work  \n","        oof[feature].iloc[val_ind] = model.predict(valid[feat_choices], num_iteration=best_iter)\n","        predictions[feature] += model.predict(test_df[feat_choices], num_iteration=best_iter)/float(n_splits)\n","\n","\n","del train_data, train, valid, val_data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T18:59:41.973397Z","iopub.status.busy":"2022-04-18T18:59:41.97286Z","iopub.status.idle":"2022-04-18T19:01:54.94096Z","shell.execute_reply":"2022-04-18T19:01:54.940197Z","shell.execute_reply.started":"2022-04-18T18:59:41.97336Z"},"trusted":true},"outputs":[],"source":["if MANUAL_RUN: \n","    oof.to_csv('oof', index = False)\n","    predictions.to_csv('predictions', index = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if MANUAL_RUN:\n","    oof = pd.read_csv('../input/satander-preds/oof')\n","    predictions = pd.read_csv('../input/satander-preds/predictions')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T22:00:39.257284Z","iopub.status.busy":"2022-04-18T22:00:39.256691Z","iopub.status.idle":"2022-04-18T22:00:39.446971Z","shell.execute_reply":"2022-04-18T22:00:39.44617Z","shell.execute_reply.started":"2022-04-18T22:00:39.257242Z"},"trusted":true},"outputs":[],"source":["# %whos"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T22:02:48.12202Z","iopub.status.busy":"2022-04-18T22:02:48.12157Z","iopub.status.idle":"2022-04-18T22:02:48.331772Z","shell.execute_reply":"2022-04-18T22:02:48.330981Z","shell.execute_reply.started":"2022-04-18T22:02:48.121965Z"},"trusted":true},"outputs":[],"source":["gc.collect()\n","test_preds = np.zeros(test_df.shape[0])"]},{"cell_type":"markdown","metadata":{},"source":["**Unused**\n","\n","The following neural network kept running out of memory and could not be trained"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["if MANUAL_RUN:\n","    epochs = 50\n","    lr_start = 0.02\n","    batch_size = 4000\n","\n","    def get_model():\n","        model = keras.models.Sequential()\n","        model.add(keras.layers.Dense(601, input_dim=601, activation='elu'))\n","        model.add(keras.layers.Dense(50, activation='elu'))\n","        model.add(keras.layers.BatchNormalization())\n","        model.add(keras.layers.Dense(40, activation='elu'))\n","        model.add(keras.layers.BatchNormalization())\n","        model.add(keras.layers.Dense(10, activation='elu'))\n","        model.add(keras.layers.BatchNormalization())\n","        model.add(keras.layers.Dense(1, activation='sigmoid'))\n","        return model\n","\n","\n","    num_vars = len(orig_features)\n","\n","    n_splits = 5\n","    skf = StratifiedKFold(n_splits, shuffle=True, random_state=SEED)\n","    \n","    nn_feat_df = pd.concat([oof, train_df], axis=1)\n","    not_feature_cols = [col for col in nn_feat_df.columns if not col.startswith('var')]\n","    nn_feat_df = nn_feat_df.drop(columns = not_feature_cols)\n","    \n","    nn_test_feat = pd.concat([predictions, test_df], axis = 1)\n","    not_feature_cols = [col for col in nn_test_feat.columns if not col.startswith('var')]\n","    nn_test_feat = nn_test_feat.drop(columns = not_feature_cols)\n","\n","    \n","    for fold, (train_ind, val_ind) in tqdm(enumerate(skf.split(oof, oof.target.values))):\n","\n","        x_train = nn_feat_df.iloc[train_ind]\n","        x_valid = nn_feat_df.iloc[val_ind]\n","        y_train = oof.iloc[train_ind]['target']\n","        y_valid = oof.iloc[val_ind]['target']\n","        \n","        optimizer = keras.optimizers.Adam(lr = 0.01, decay = 0.00001)\n","        model = get_model()\n","        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n","        model.fit(x_train, y_train, validation_data=(x_valid, y_valid), epochs=epochs, verbose=2, batch_size=batch_size)\n","\n","        test_preds += model.predict(nn_test_feat, batch_size=batch_size)[:,0]/float(n_splits)   "]},{"cell_type":"markdown","metadata":{},"source":["In order to minimize overfitting and utilize the similarity of patterns across different original features, convolutional layers were used. Various different CNN structures along with hyperparameter values were tried. But none achieved the improvement in performance as the CNN built in a kernal from kaggle. This particular set up seems to be the 'magic' combination that is required for the output from the 200 lgb models. Hence, the following CNN is same as the cited kernel but has been re-written by us.  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# FROM NAWID SAYED'S KERNEL\n","# https://www.kaggle.com/code/nawidsayed/lightgbm-and-cnn-3rd-place-solution\n","\n","epochs = 100\n","lr_start = 0.02\n","batch_size = 4000\n","num_preds = 3\n","orig_feat_len = len(orig_features)\n","\n","# reorder the features to have a variable, its value count and its prediction from lgb next to each other \n","def make_nn_feats(predictions, df):\n","    feat_cols = [col for col in df.columns if col.startswith('var')]\n","    all_feats = [predictions[orig_features].values, df[feat_cols].values]\n","    ind_list = []\n","    for i in range(orig_feat_len):\n","        ind = np.arange(num_preds)*orig_feat_len + i\n","        ind_list.append(ind)\n","    ordered_ind = np.concatenate(ind_list)\n","    return np.concatenate(all_feats, axis = 1)[:, ordered_ind]\n","\n","# The architecture is chosen to ensure feature independence until the last dense layer.  \n","# Batch normalization layers regularize the model well and improve the final score. \n","# The model has a total of 2733 trainable parameters which is sufficiently low to not run out of memory and train fast.\n","# Having low number of trainable parameters also prevents overfitting. \n","\n","def get_model():\n","    inputs = keras.layers.Input((orig_feat_len*num_preds,))\n","    x = keras.layers.Reshape((orig_feat_len*num_preds,1))(inputs)\n","    x = keras.layers.Conv1D(32,num_preds,strides=num_preds, activation='elu')(x)\n","    x = keras.layers.BatchNormalization()(x)\n","    x = keras.layers.Conv1D(24,1, activation='elu')(x)\n","    x = keras.layers.BatchNormalization()(x)\n","    x = keras.layers.Conv1D(16,1, activation='elu')(x)\n","    x = keras.layers.BatchNormalization()(x)\n","    x = keras.layers.Conv1D(4,1, activation='elu')(x)\n","    x = keras.layers.Flatten()(x)\n","    x = keras.layers.Reshape((orig_feat_len*4,1))(x)\n","    x = keras.layers.AveragePooling1D(2)(x)\n","    x = keras.layers.Flatten()(x)\n","    x = keras.layers.BatchNormalization()(x)\n","    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n","    return keras.Model(inputs=inputs, outputs=outputs)\n","\n","get_model().summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["if not MANUAL_RUN:\n","    num_vars = len(orig_features)\n","\n","    n_splits = 5\n","    skf = StratifiedKFold(n_splits, shuffle=True, random_state=SEED)\n","    \n","    nn_train = make_nn_feats(oof, train_df)\n","    nn_test = make_nn_feats(predictions, test_df)\n","\n","    del train_df, test_df\n","    \n","#     without learning rate scheduler, the score was about 0.01 lower\n","    def get_lr(epoch):\n","        if epoch <= epochs*0.8:\n","            return lr_start\n","        else:\n","            return lr_start * 0.1\n","\n","\n","    for fold, (train_ind, val_ind) in tqdm(enumerate(skf.split(oof, oof.target.values))):\n","        x_train = nn_train[train_ind, :]\n","        x_valid = nn_train[val_ind, :]\n","        y_train = oof.iloc[train_ind]['target']\n","        y_valid = oof.iloc[val_ind]['target']\n","        \n","        optimizer = keras.optimizers.Adam(learning_rate = lr_start, decay = 0.00001)\n","        model = get_model()\n","        callbacks = [keras.callbacks.LearningRateScheduler(get_lr)]\n","        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n","        model.fit(x_train, y_train, validation_data=(x_valid, y_valid), epochs=epochs, batch_size=batch_size, callbacks=callbacks)\n","        test_preds += model.predict(nn_test, batch_size=2000)[:,0]/float(n_splits)\n","        gc.collect()\n","\n","    del nn_train, nn_test, x_train, x_valid"]},{"cell_type":"markdown","metadata":{},"source":["Combining the predictions using logistic regression"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T17:00:59.568963Z","iopub.status.busy":"2022-04-17T17:00:59.568663Z","iopub.status.idle":"2022-04-17T17:01:26.262502Z","shell.execute_reply":"2022-04-17T17:01:26.261634Z","shell.execute_reply.started":"2022-04-17T17:00:59.568931Z"},"trusted":true},"outputs":[],"source":["if MANUAL_RUN:\n","    lg_feats = orig_features + ['var_ones']\n","\n","    lr = sm.Logit(oof['target'], oof[lg_feats])\n","    lr = lr.fit()\n","\n","    if MANUAL_RUN:\n","        ensemble_preds = lr.predict(oof[lg_feats])\n","        roc_auc_score(oof['target'],ensemble_preds)\n","    \n","    test_preds = lr.predict(predictions[lg_feats])"]},{"cell_type":"markdown","metadata":{},"source":["Combining the predictions using sum of logit of the individual predictions. This was the best way to combine the results without the CNN"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T18:55:47.947189Z","iopub.status.busy":"2022-04-18T18:55:47.946926Z","iopub.status.idle":"2022-04-18T18:55:49.036656Z","shell.execute_reply":"2022-04-18T18:55:49.035836Z","shell.execute_reply.started":"2022-04-18T18:55:47.947161Z"},"trusted":true},"outputs":[],"source":["if MANUAL_RUN:\n","    predictions_new = predictions.copy()\n","    predictions_new = logit(predictions_new.drop(columns=['ID_code', 'var_ones']))\n","    test_preds = np.sum(predictions_new[[col for col in predictions_new.columns if col.startswith('var')]].values, axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["Combing the results as a sum/mean of individual predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T17:08:15.881589Z","iopub.status.busy":"2022-04-17T17:08:15.881334Z","iopub.status.idle":"2022-04-17T17:08:16.291596Z","shell.execute_reply":"2022-04-17T17:08:16.290864Z","shell.execute_reply.started":"2022-04-17T17:08:15.881561Z"},"trusted":true},"outputs":[],"source":["if MANUAL_RUN:\n","    test_preds = np.sum(predictions[[col for col in predictions.columns if col.startswith('var')]].values, axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["Generating the submission file"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T22:29:34.558434Z","iopub.status.busy":"2022-04-18T22:29:34.558141Z","iopub.status.idle":"2022-04-18T22:29:35.247778Z","shell.execute_reply":"2022-04-18T22:29:35.246949Z","shell.execute_reply.started":"2022-04-18T22:29:34.558401Z"},"trusted":true},"outputs":[],"source":["submission = pd.DataFrame({'ID_code':pd.Series(predictions['ID_code']),'target':pd.Series(test_preds)})\n","submission.to_csv('submission.csv',index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
